{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('./data/UMLS/full_cui2def.json','r') as f:\n",
    "    cui2def = json.load(f)\n",
    "\n",
    "updated_cui2def = {}\n",
    "total_defs = 0\n",
    "total_updated_defs = 0\n",
    "for cui, def_list in cui2def.items():\n",
    "    total_defs += len(def_list)\n",
    "    if cui not in updated_cui2def:\n",
    "        updated_cui2def[cui] = []\n",
    "        unique_defs = set() \n",
    "        for definition in def_list:\n",
    "            unique_defs.add(definition.lower())\n",
    "        for definition in unique_defs:\n",
    "            updated_cui2def[cui].append(definition)\n",
    "        total_updated_defs += len(unique_defs)\n",
    "\n",
    "print(len(updated_cui2def),len(cui2def))\n",
    "print(total_defs, total_updated_defs)\n",
    "\n",
    "with open('./data/UMLS/cui2def.json','w') as f:\n",
    "    json.dump(updated_cui2def, f)\n",
    "\n",
    "with open('./data/UMLS/cui2def.json','r') as f:\n",
    "    cui2def = json.load(f)\n",
    "\n",
    "for idx,(k,v) in enumerate(cui2def.items()):\n",
    "    print(k,v)\n",
    "    if idx == 10:\n",
    "        break\n",
    "    \n",
    "with open('./data/UMLS/cui2syn.json','r') as f:\n",
    "    cui2syn = json.load(f)\n",
    "\n",
    "updated_cui2syn = {}\n",
    "total_syn = 0\n",
    "total_updated_syn = 0\n",
    "for cui, syn_list in cui2syn.items():\n",
    "    total_syn += len(syn_list)\n",
    "    if cui not in updated_cui2syn:\n",
    "        updated_cui2syn[cui] = []\n",
    "        unique_syns = set()\n",
    "        for syn in syn_list:\n",
    "            unique_syns.add(syn.lower())\n",
    "        for syn in unique_syns:\n",
    "            updated_cui2syn[cui].append(syn)\n",
    "        total_updated_syn += len(unique_syns)\n",
    "print(total_syn, total_updated_syn)\n",
    "print(len(cui2syn),len(updated_cui2syn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/UMLS/cui2syn.json','r') as f:\n",
    "    cui2syn = json.load(f)\n",
    "    \n",
    "with open('./data/UMLS/cui2def.json','r') as f:\n",
    "    cui2def = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227407\r"
     ]
    }
   ],
   "source": [
    "with open('./data/UMLS/sub_dictionary.txt','w',encoding='utf-8') as f:\n",
    "    for idx, (cui, defs) in enumerate(cui2def.items()):\n",
    "        print(idx, end='\\r')\n",
    "        if cui in cui2syn:\n",
    "            name = cui2syn[cui][0]\n",
    "            for definition in defs:\n",
    "                    definiton = definition.encode(\"utf-8\")\n",
    "                    f.write(cui + '\\t' + name + '\\t' + definition + '\\n')\n",
    "            # for syn in cui2syn[cui]:\n",
    "            #     for definition in defs:\n",
    "            #         definiton = definition.encode(\"utf-8\")\n",
    "            #         f.write(cui + '\\t' + syn + '\\t' + definition + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('./data/UMLS/sub_dictionary.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181409 45353 56691\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(lines, test_size=0.2, train_size=0.8, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, train_size=0.8, random_state=42)\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(f, data_split):\n",
    "    for line in data_split:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "with open('./data/pretrain/sub_train_dictionary.txt','w') as f:\n",
    "    write_to_file(f, train)\n",
    "\n",
    "with open('./data/pretrain/sub_val_dictionary.txt','w') as f:\n",
    "    write_to_file(f, val)\n",
    "\n",
    "with open('./data/pretrain/sub_test_dictionary.txt','w') as f:\n",
    "    write_to_file(f, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "el",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
