{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import UMLSKGDataset\n",
    "dataset = UMLSKGDataset(\"/share/project/biomed/hcd/UMLS/processed_data/eng_rel_subset.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81495\n",
      "193\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.entities))\n",
    "print(len(dataset.relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/kge/ent2idx.pkl','wb') as handler:\n",
    "    pickle.dump(dataset.entity_to_id, handler)\n",
    "    \n",
    "with open('./data/kge/rel2idx.pkl','wb') as handler:\n",
    "    pickle.dump(dataset.relation_to_id, handler)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transe import TransE\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    \"\"\"To setup as many loggers as you want\"\"\"\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "    handler = logging.FileHandler(log_file, mode='a')        \n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(consoleHandler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_dataloader, valid_dataloader, logger, num_epochs, learning_rate):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model = None\n",
    "    best_val = 0\n",
    "    #print(\"I am here\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "            positive_samples, negative_samples = batch\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(positive_samples.to(device), negative_samples.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        valid_loss = evaluate(model, valid_dataloader, device)\n",
    "        #print(\"printing log\")\n",
    "        logger.info(\"Epoch {}, Train Loss: {}, Valid Loss: {}\".format(epoch,train_loss,valid_loss))\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(valid_loss)\n",
    "        #best_val = valid_loss\n",
    "        if best_val == 0:\n",
    "            best_val = valid_loss\n",
    "        if valid_loss < best_val:\n",
    "            best_model = {'model': model.state_dict(),\n",
    "              'optimizer': optimizer.state_dict()}\n",
    "            torch.save(best_model, './model_ckpts/transE/best_model.pt')\n",
    "        if (epoch+1)%10 == 0:\n",
    "            checkpoint = {'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()}\n",
    "            torch.save(checkpoint, './model_ckpts/transE/model_ckpt_'+str(epoch)+'.pt')\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(dataloader,desc = \"Validation\")):\n",
    "            positive_samples, negative_samples = batch\n",
    "            loss = model.compute_loss(positive_samples.to(device), negative_samples.to(device))\n",
    "            total_loss += loss.item()\n",
    "    total_loss /= len(dataloader)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 837/837 [02:39<00:00,  5.25it/s]\n",
      "Validation: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s]\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "Training: 100%|██████████| 837/837 [02:39<00:00,  5.26it/s]\n",
      "Validation: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s]\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "Training:  67%|██████▋   | 564/837 [01:47<00:52,  5.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m train_and_evaluate(model, train_dataloader, test_dataloader, logger, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_dataloader, valid_dataloader, logger, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m     17\u001b[0m     positive_samples, negative_samples \u001b[39m=\u001b[39m batch\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/share/project/biomed/hcd/Masked_EL/kg_dataset.py:38\u001b[0m, in \u001b[0;36mUMLSKGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m corrupt_tail \u001b[39m=\u001b[39m tail\n\u001b[1;32m     36\u001b[0m \u001b[39mwhile\u001b[39;00m corrupt_tail \u001b[39m==\u001b[39m tail:\n\u001b[1;32m     37\u001b[0m     \u001b[39m# Randomly select an entity to corrupt the tail\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     corrupt_tail \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentities), p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentity_freq \u001b[39m/\u001b[39;49m np\u001b[39m.\u001b[39;49msum(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentity_freq))\n\u001b[1;32m     39\u001b[0m negative_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([head, relation, corrupt_tail])\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mLongTensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtriples[idx]), negative_sample\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_entities = len(dataset.entities)\n",
    "n_rels = len(dataset.relations)\n",
    "n_embs = 256\n",
    "margin = 1\n",
    "model = TransE(n_entities, n_rels, n_embs, margin)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "logger = setup_logger('TransE_logger', './logs/transE.log')\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "train_loss, val_loss = train_and_evaluate(model, train_dataloader, test_dataloader, logger, num_epochs=50, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/UMLS/kg.txt',sep='\\t', header= None, names=['h','r','t'])\n",
    "# Group by 'Column1' and count the occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778355 1090990\n"
     ]
    }
   ],
   "source": [
    "r_counts = df.groupby('r').transform('size')\n",
    "# Boolean indexing to filter rows\n",
    "filtered_df = df[r_counts >= 10000]\n",
    "# Display the filtered DataFrame\n",
    "print(len(filtered_df),len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/UMLS/full_cui2def.json','r') as f:\n",
    "    cui2def = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227408 227408\n",
      "284363 284106\n"
     ]
    }
   ],
   "source": [
    "updated_cui2def = {}\n",
    "total_defs = 0\n",
    "total_updated_defs = 0\n",
    "for cui, def_list in cui2def.items():\n",
    "    total_defs += len(def_list)\n",
    "    if cui not in updated_cui2def:\n",
    "        updated_cui2def[cui] = []\n",
    "        unique_defs = set() \n",
    "        for definition in def_list:\n",
    "            unique_defs.add(definition.lower())\n",
    "        for definition in unique_defs:\n",
    "            updated_cui2def[cui].append(definition)\n",
    "        total_updated_defs += len(unique_defs)\n",
    "\n",
    "print(len(updated_cui2def),len(cui2def))\n",
    "print(total_defs, total_updated_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/UMLS/cui2def.json','w') as f:\n",
    "    json.dump(updated_cui2def, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C0000039 ['synthetic phospholipid used in liposomes and lipid bilayers to study biological membranes. it is also a major constituent of pulmonary surfactants.']\n",
      "C0000052 ['in glycogen or amylopectin synthesis, the enzyme that catalyzes the transfer of a segment of a 1,4-alpha-glucan chain to a primary hydroxy group in a similar glucan chain. ec 2.4.1.18.']\n",
      "C0000084 ['found in various tissues, particularly in four blood-clotting proteins including prothrombin, in kidney protein, in bone protein, and in the protein present in various ectopic calcifications.']\n",
      "C0000096 ['a potent cyclic nucleotide phosphodiesterase inhibitor; due to this action, the compound increases cyclic amp and cyclic gmp in tissue and thereby activates cyclic nucleotide-regulated protein kinases']\n",
      "C0000097 ['a dopaminergic neurotoxic compound which produces irreversible clinical, chemical, and pathological alterations that mimic those found in parkinson disease.', '1-methyl-4-phenyl-1,2,5,6-tetrahydropyridine, a toxic contaminant found in the abused drug methylenedioxymethamphetamine (\"ecstasy\") which causes parkinsonlike symptoms.']\n",
      "C0000098 ['an active neurotoxic metabolite of 1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine. the compound reduces dopamine levels, inhibits the biosynthesis of catecholamines, depletes cardiac norepinephrine and inactivates tyrosine hydroxylase. these and other toxic effects lead to cessation of oxidative phosphorylation, atp depletion, and cell death. the compound, which is related to paraquat, has also been used as an herbicide.']\n",
      "C0000102 ['a suspected industrial carcinogen (and listed as such by osha). its n-hydroxy metabolite is strongly carcinogenic and mutagenic.']\n",
      "C0000103 ['a tool for the study of liver damage which causes bile stasis and hyperbilirubinemia acutely and bile duct hyperplasia and biliary cirrhosis chronically, with changes in hepatocyte function. it may cause skin and kidney damage.']\n",
      "C0000107 ['an angiotensin ii analog which acts as a highly specific inhibitor of angiotensin type 1 receptor.']\n",
      "C0000119 ['a group of corticosteroids bearing a hydroxy group at the 11-position.']\n",
      "C0000139 ['a stable derivative of prostaglandin e2 (pge2) with potential hematopoietic activity. administration of 16,16 dimethyl-prostaglandin e2 (dmpge2) appears to lead to increased formation of hematopoietic stem and progenitor cells. even though the exact mechanism of action has yet to be fully elucidated, this agent may stimulate hematopoiesis by activating the wnt signaling pathway, which increases cellular levels of beta-catenin, a subunit of the cadherin protein complex. check for \"https://www.cancer.gov/about-cancer/treatment/clinical-trials/intervention/c84850\" active clinical trials using this agent. (\"http://ncit.nci.nih.gov/ncitbrowser/conceptreport.jsp?dictionary=nci%20thesaurus&code=c84850\" nci thesaurus)', 'a synthetic prostaglandin e analog that protects the gastric mucosa, prevents ulceration, and promotes the healing of peptic ulcers. the protective effect is independent of acid inhibition. it is also a potent inhibitor of pancreatic function and growth of experimental tumors.', 'a stable derivative of prostaglandin e2 (pge2) with potential hematopoietic activity. administration of 16,16 dimethyl-prostaglandin e2 (dmpge2) appears to lead to increased formation of hematopoietic stem and progenitor cells. even though the exact mechanism of action has yet to be fully elucidated, this agent may stimulate hematopoiesis by activating the wnt signaling pathway, which increases cellular levels of beta-catenin, a subunit of the cadherin protein complex.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./data/UMLS/cui2def.json','r') as f:\n",
    "    cui2def = json.load(f)\n",
    "\n",
    "for idx,(k,v) in enumerate(cui2def.items()):\n",
    "    print(k,v)\n",
    "    if idx == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/UMLS/cui2syn.json','r') as f:\n",
    "    cui2syn = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9508795 9508795\n",
      "4548855 4548855\n"
     ]
    }
   ],
   "source": [
    "updated_cui2syn = {}\n",
    "total_syn = 0\n",
    "total_updated_syn = 0\n",
    "for cui, syn_list in cui2syn.items():\n",
    "    total_syn += len(syn_list)\n",
    "    if cui not in updated_cui2syn:\n",
    "        updated_cui2syn[cui] = []\n",
    "        unique_syns = set()\n",
    "        for syn in syn_list:\n",
    "            unique_syns.add(syn.lower())\n",
    "        for syn in unique_syns:\n",
    "            updated_cui2syn[cui].append(syn)\n",
    "        total_updated_syn += len(unique_syns)\n",
    "print(total_syn, total_updated_syn)\n",
    "print(len(cui2syn),len(updated_cui2syn))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/UMLS/cui2syn.json','w') as f:\n",
    "    json.dump(updated_cui2syn,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\r"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u03b3' in position 0: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m syn \u001b[39min\u001b[39;00m updated_cui2syn[cui]:\n\u001b[1;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m definition \u001b[39min\u001b[39;00m defs:\n\u001b[0;32m----> 7\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(syn\u001b[39m.\u001b[39;49mencode(\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mignore\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m definition\u001b[39m.\u001b[39;49mencode(\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mignore\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u03b3' in position 0: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "with open('./data/UMLS/dictionary.txt','w') as f:\n",
    "    for idx, (cui, defs) in enumerate(updated_cui2def.items()):\n",
    "        print(idx, end='\\r')\n",
    "        if cui in updated_cui2syn:\n",
    "            for syn in updated_cui2syn[cui]:\n",
    "                for definition in defs:\n",
    "                    \n",
    "                    f.write(syn + '\\t' + definition + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeshel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
