{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kg_dataset import UMLSKGDataset\n",
    "dataset = UMLSKGDataset(\"/share/project/biomed/hcd/UMLS/processed_data/eng_rel_subset.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81495\n",
      "193\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.entities))\n",
    "print(len(dataset.relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/kge/ent2idx.pkl','wb') as handler:\n",
    "    pickle.dump(dataset.entity_to_id, handler)\n",
    "    \n",
    "with open('./data/kge/rel2idx.pkl','wb') as handler:\n",
    "    pickle.dump(dataset.relation_to_id, handler)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transe import TransE\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "    \"\"\"To setup as many loggers as you want\"\"\"\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "    handler = logging.FileHandler(log_file, mode='a')        \n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(consoleHandler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_dataloader, valid_dataloader, logger, num_epochs, learning_rate):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model = None\n",
    "    best_val = 0\n",
    "    #print(\"I am here\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "            positive_samples, negative_samples = batch\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(positive_samples.to(device), negative_samples.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        valid_loss = evaluate(model, valid_dataloader, device)\n",
    "        #print(\"printing log\")\n",
    "        logger.info(\"Epoch {}, Train Loss: {}, Valid Loss: {}\".format(epoch,train_loss,valid_loss))\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(valid_loss)\n",
    "        #best_val = valid_loss\n",
    "        if best_val == 0:\n",
    "            best_val = valid_loss\n",
    "        if valid_loss < best_val:\n",
    "            best_model = {'model': model.state_dict(),\n",
    "              'optimizer': optimizer.state_dict()}\n",
    "            torch.save(best_model, './model_ckpts/transE/best_model.pt')\n",
    "        if (epoch+1)%10 == 0:\n",
    "            checkpoint = {'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()}\n",
    "            torch.save(checkpoint, './model_ckpts/transE/model_ckpt_'+str(epoch)+'.pt')\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(dataloader,desc = \"Validation\")):\n",
    "            positive_samples, negative_samples = batch\n",
    "            loss = model.compute_loss(positive_samples.to(device), negative_samples.to(device))\n",
    "            total_loss += loss.item()\n",
    "    total_loss /= len(dataloader)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 837/837 [02:39<00:00,  5.25it/s]\n",
      "Validation: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s]\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "2023-05-15 21:33:32,217 INFO Epoch 0, Train Loss: 0.45198343822392084, Valid Loss: 0.3522786039681662\n",
      "Training: 100%|██████████| 837/837 [02:39<00:00,  5.26it/s]\n",
      "Validation: 100%|██████████| 210/210 [00:39<00:00,  5.36it/s]\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "2023-05-15 21:36:50,554 INFO Epoch 1, Train Loss: 0.29715290546346024, Valid Loss: 0.3008225204689162\n",
      "Training:  67%|██████▋   | 564/837 [01:47<00:52,  5.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m train_and_evaluate(model, train_dataloader, test_dataloader, logger, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_dataloader, valid_dataloader, logger, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m     17\u001b[0m     positive_samples, negative_samples \u001b[39m=\u001b[39m batch\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/share/project/biomed/envs/zeshel/lib/python3.8/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/share/project/biomed/hcd/Masked_EL/kg_dataset.py:38\u001b[0m, in \u001b[0;36mUMLSKGDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m corrupt_tail \u001b[39m=\u001b[39m tail\n\u001b[1;32m     36\u001b[0m \u001b[39mwhile\u001b[39;00m corrupt_tail \u001b[39m==\u001b[39m tail:\n\u001b[1;32m     37\u001b[0m     \u001b[39m# Randomly select an entity to corrupt the tail\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     corrupt_tail \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentities), p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentity_freq \u001b[39m/\u001b[39;49m np\u001b[39m.\u001b[39;49msum(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentity_freq))\n\u001b[1;32m     39\u001b[0m negative_sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([head, relation, corrupt_tail])\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mLongTensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtriples[idx]), negative_sample\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_entities = len(dataset.entities)\n",
    "n_rels = len(dataset.relations)\n",
    "n_embs = 256\n",
    "margin = 1\n",
    "model = TransE(n_entities, n_rels, n_embs, margin)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "logger = setup_logger('TransE_logger', './logs/transE.log')\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "train_loss, val_loss = train_and_evaluate(model, train_dataloader, test_dataloader, logger, num_epochs=50, learning_rate=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeshel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
